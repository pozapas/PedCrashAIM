{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''\n",
    "# Pedestrian crash severity analysis using Adaptive Interpretive Modeling (AIM)\n",
    "# Author:   Amir Rafe (amir.rafe@usu.edu)\n",
    "# File:     AIM_TabNet.ipynb\n",
    "# Date:     Spring 2024\n",
    "# Version:  1.47  \n",
    "# About:    AIM pipeline to pedestrian crash severity analysis using TabNet method\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.cluster import KMeans\n",
    "import shap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "Ped_df = pd.read_csv('df.csv')\n",
    "\n",
    "# List of features to use for modeling\n",
    "features = ['PersonType','Sex','AgeText','Aggressive','AlcoholSuspected','AlcResult',\n",
    "                       'UrbanRural','FunctionClass','CommercialMotorVehInvolved','Light',\n",
    "                       'Weather','RoadwaySurf','DisregardTrafficControl','DistractedDriving',\n",
    "                       'DomesticAnimalRelated','DrowsyDriving','DrugsSuspected','OlderDriverInvolved',\n",
    "                       'TeenageDriverInvolved','DUI','HeavyTruckInvolved','OverturnRollover','RightTurn',\n",
    "                       'TransitVehicleInvolved','HolidayCrash','HolidayCrashYN','Intersection',\n",
    "                       'LeftUTurnInvolved','VerticalAlignment','WorkZoneInvolved','WrongWayDriving']\n",
    "\n",
    "# Features for confounding factors\n",
    "confounders = ['AgeText', 'AlcResult', 'Light', 'Weather', 'OlderDriverInvolved', 'TeenageDriverInvolved']\n",
    "\n",
    "# Target variable\n",
    "target = 'Severity'\n",
    "\n",
    "# Convert all columns to numeric, handling errors by coercing to NaNs\n",
    "Ped_df = Ped_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "Ped_df.fillna(0, inplace=True)\n",
    "\n",
    "# Convert the 'Severity' column to integer type\n",
    "Ped_df['Severity'] = Ped_df['Severity'].astype(int)\n",
    "\n",
    "# Feature representing a continuous variable\n",
    "numerical_features = ['AgeText']\n",
    "\n",
    "# Determine categorical features by subtracting the set of numerical features from the set of all features\n",
    "categorical_features = list(set(features) - set(numerical_features))\n",
    "\n",
    "# Preprocessing pipeline for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing pipeline to the data\n",
    "X = preprocessor.fit_transform(Ped_df[features])\n",
    "y = Ped_df[target].values\n",
    "\n",
    "# Create a new column in 'Ped_df' for spatial clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(Ped_df[['LAT', 'LNG']])\n",
    "Ped_df['spatial_cluster'] = kmeans.labels_\n",
    "\n",
    "# Stratify by 'AgeText' and 'Light' confounders\n",
    "Ped_df['AgeText_binned'] = pd.qcut(Ped_df['AgeText'], 4, labels=False, duplicates='drop')\n",
    "Ped_df['composite_key'] = Ped_df['spatial_cluster'].astype(str) + \"_\" + Ped_df['AgeText_binned'].astype(str) + \"_\" + Ped_df['Light'].astype(str)\n",
    "Ped_df['composite_key_encoded'] = pd.factorize(Ped_df['composite_key'])[0]\n",
    "\n",
    "# Split the dataset 80-20\n",
    "X = preprocessor.fit_transform(Ped_df[features])\n",
    "y = Ped_df[target].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=Ped_df['composite_key_encoded'], random_state=42)\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Placeholder for accumulated accuracies across folds\n",
    "    fold_accuracies = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, y_train_fold = X_train[train_index], y_train[train_index]\n",
    "        X_test_fold, y_test_fold = X_train[test_index], y_train[test_index]\n",
    "        \n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_fold_smote, y_train_fold_smote = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Suggest hyperparameters within the trial\n",
    "        n_d = trial.suggest_int('n_d', 16, 64)\n",
    "        n_a = trial.suggest_int('n_a', 16, 64)\n",
    "        n_steps = trial.suggest_int('n_steps', 1, 10)\n",
    "        gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "        n_independent = trial.suggest_int('n_independent', 1, 10)\n",
    "        n_shared = trial.suggest_int('n_shared', 1, 10)\n",
    "        lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-5, 1e-1)\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "\n",
    "        # Initialize and train TabNetClassifier with suggested hyperparameters\n",
    "        clf = TabNetClassifier(\n",
    "            n_d=n_d, n_a=n_a, n_steps=n_steps,\n",
    "            gamma=gamma, n_independent=n_independent, n_shared=n_shared,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=lr),\n",
    "            scheduler_params={\"step_size\":10, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            mask_type='entmax', input_dim=X_train_fold.shape[1]\n",
    "        )\n",
    "\n",
    "        clf.fit(X_train_fold_smote, y_train_fold_smote, \n",
    "                eval_set=[(X_test_fold, y_test_fold)], \n",
    "                eval_name=['valid'], eval_metric=['accuracy'], \n",
    "                max_epochs=100, patience=20, batch_size=256, \n",
    "                virtual_batch_size=128, num_workers=0, drop_last=False)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        y_pred = clf.predict(X_test_fold)\n",
    "        \n",
    "        # Calculate and store the accuracy for the current fold\n",
    "        fold_accuracies.append(accuracy_score(y_test_fold, y_pred))\n",
    "    \n",
    "    # The objective is to maximize the average accuracy across all folds\n",
    "    average_accuracy = np.mean(fold_accuracies)\n",
    "    return average_accuracy\n",
    "\n",
    "# Create an Optuna study that seeks to maximize the average accuracy across folds\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# The best hyperparameters\n",
    "best_params = study.best_params\n",
    "print('Best hyperparameters:', best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_train_model(X_train_fold_smote, y_train_fold_smote):\n",
    "    \"\"\"\n",
    "    Create and train a TabNet model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train_fold_smote: Training features\n",
    "    - y_train_fold_smote: Training labels\n",
    "\n",
    "    Returns:\n",
    "    - clf: Trained TabNet classifier\n",
    "    \"\"\"\n",
    "    # Initialize TabNetClassifier with predefined hyperparameters\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=53, n_a=58, n_steps=1,\n",
    "        gamma=1.9526677094303813, n_independent=8, n_shared=6,\n",
    "        lambda_sparse=0.023989318086231295, momentum=0.3, clip_value=2.,\n",
    "        optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=0.007566831910358642),\n",
    "        scheduler_params={\"step_size\":10, \"gamma\":0.9}, scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='entmax', input_dim=X_train_fold_smote.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the model on the provided training set\n",
    "    clf.fit(\n",
    "        X_train_fold_smote, y_train_fold_smote,\n",
    "        eval_set=[(X_train_fold_smote, y_train_fold_smote)],\n",
    "        eval_name=['train'],\n",
    "        eval_metric=['accuracy'],\n",
    "        max_epochs=1000, patience=20,\n",
    "        batch_size=256, virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        weights=1,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def predict(model, X_test_fold):\n",
    "    \"\"\"\n",
    "    Make predictions using a trained TabNet model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained TabNet classifier\n",
    "    - X_test_fold: Test features\n",
    "\n",
    "    Returns:\n",
    "    - preds: Predictions as integer values\n",
    "    \"\"\"\n",
    "    # Predict using the model and round off to get integer class labels\n",
    "    preds = model.predict(X_test_fold)\n",
    "    return np.round(preds).astype(int)\n",
    "\n",
    "# Initialize variables to keep track of the best model\n",
    "n_models = 1\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Train and evaluate n_models TabNet models\n",
    "for i in range(n_models):\n",
    "    # Create bootstrap sample to simulate resampling\n",
    "    sample_idxs = np.random.choice(len(X_train_fold_smote), size=len(X_train_fold_smote), replace=True)\n",
    "    X_sample, y_sample = X_train_fold_smote[sample_idxs], y_train_fold_smote[sample_idxs]\n",
    "    \n",
    "    # Train model on the bootstrap sample\n",
    "    model = create_train_model(X_sample, y_sample)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = predict(model, X_test_fold)\n",
    "    \n",
    "    # Calculate accuracy and other performance metrics\n",
    "    accuracy = accuracy_score(y_test_fold, y_pred)\n",
    "    cm = confusion_matrix(y_test_fold, y_pred)\n",
    "    cr = classification_report(y_test_fold, y_pred)\n",
    "    \n",
    "    # Display performance metrics\n",
    "    print(f'Model {i+1}:')\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Confusion matrix:')\n",
    "    print(cm)\n",
    "    print('Classification report:')\n",
    "    print(cr)\n",
    "    print('----------------------------------')\n",
    "\n",
    "    # If the current model is the best so far, update best_model and best_accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "\n",
    "# Save the best model to a file\n",
    "dump(best_model, 'best_model.joblib')\n",
    "\n",
    "# Get feature importances from the best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Map feature names to their importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,  \n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Save feature importances to a CSV file\n",
    "importance_df.to_csv('feature_importances.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP interpretation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a background dataset from features. This subset is used as a reference point to compute SHAP values.\n",
    "# 150 samples are typically enough to approximate the SHAP values without being too computationally intensive.\n",
    "background = shap.sample(X_train_fold_smote, 150)\n",
    "\n",
    "# Initialize a SHAP explainer object. This uses the KernelExplainer, which is model-agnostic and can be used \n",
    "# with any machine learning model. The explainer requires a prediction function and a background dataset.\n",
    "# Here, `best_model.predict_proba` is passed to compute SHAP values for classification models, which outputs\n",
    "# probabilities for each class. The background dataset helps in comparing the feature's effect when present vs. absent.\n",
    "explainer = shap.KernelExplainer(best_model.predict_proba, background)\n",
    "\n",
    "# Compute SHAP values for each feature in dataset X. SHAP values quantify the impact of each feature on the model's prediction.\n",
    "# This computation is done for all instances in X, allowing to interpret the model's behavior. The `n_jobs=-1` parameter\n",
    "# enables parallel computation, using all available CPUs to speed up the calculation.\n",
    "shap_values = explainer.shap_values(background, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap summary plot (feature importance)\n",
    "\n",
    "features2 = ['PersonType','Sex','Age','Aggressive','DrugResult','AlcResult','UrbanRural','FunctionClass','CommercialVehInvolved',\n",
    "                       'LightingCondition','Weather','RoadwaySurf','DisregardTrafficControl','DistractedDriving','DomesticAnimalRelated',\n",
    "                       'DrowsyDriving','DrugsSuspected','OlderDriverInvolved','TeenageDriverInvolved','DUI','HeavyTruckInvolved',\n",
    "                       'OverturnRollover','RightTurnInvolved','TransitVehicleInvolved','Aggressive',\n",
    "                       'HolidayCrashYN','Intersection','LeftUTurnInvolved','VerticalAlignment','WorkZoneInvolved','WrongWayDriving']\n",
    "\n",
    "shap.summary_plot(shap_values, X_train, feature_names=features2 , plot_type= 'bar' , plot_size=(12,12) ,  class_names= {\n",
    "    0: 'Possible injury',\n",
    "    1: 'Minor injury',\n",
    "    2: 'No injury/PDO',\n",
    "    3: 'Serious injury',\n",
    "    4: 'Fatal'\n",
    "},show=False)\n",
    "\n",
    "# Get the current figure and axes objects\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "# Modifying main plot parameters\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"mean(|SHAP value|) (average impact on model output magnitude)\", fontsize=16 ,  labelpad=15)\n",
    "\n",
    "# Get colorbar\n",
    "plt.legend(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary_TabNet.png\",dpi=300) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap summary plot (dot plot)\n",
    "\n",
    "shap.summary_plot(shap_values[0], X_train, feature_names=features2 , plot_type= 'dot' , plot_size=(10,10),show=False)\n",
    "\n",
    "# Get the current figure and axes objects\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "# Modifying main plot parameters\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", fontsize=16  , labelpad=15)\n",
    "\n",
    "# Get colorbar\n",
    "cb_ax = fig.axes[1] \n",
    "cb_ax.tick_params(labelsize=16)\n",
    "cb_ax.set_ylabel(\"Feature value\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary_TabNet_Possible.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-hoc analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDP\n",
    "X_df = pd.DataFrame(X_train, columns=features2)\n",
    "shap.dependence_plot('LightingCondition', shap_values[4], X_df, interaction_index='Age' , dot_size=2, x_jitter=0.5 , show=False)\n",
    "plt.savefig(\"PDP_TabNet_Fatal.png\",dpi=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n",
    "print('Classification report:')\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensetivity analysis \n",
    "\n",
    "# Light' feature index \n",
    "light_feature_index = features.index('Light')\n",
    "\n",
    "# Function to simulate the impact of a policy intervention on the 'Light' feature\n",
    "def simulate_policy_intervention_and_plot_shap(X_test, shap_values, light_feature_index, severity_classes, intervention_mapping):\n",
    "    # Make a copy of the original dataset\n",
    "    X_intervention = np.copy(X_test)\n",
    "    \n",
    "    # Apply the policy intervention simulation: change 'Dark-Not lighted' to 'Dark-Lighted'\n",
    "    # Replace all instances of '1' with '0' in the 'Light' feature\n",
    "    X_intervention[X_intervention[:, light_feature_index] == intervention_mapping['from']] = intervention_mapping['to']\n",
    "    \n",
    "    # Calculate the SHAP values for the dataset with the intervention\n",
    "    shap_values_intervention = [explainer.shap_values(X_intervention)[i] for i in range(5)]\n",
    "    \n",
    "    # Plotting the SHAP values distribution before and after the intervention\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    fig.suptitle('SHAP Values Distribution for Policy Intervention on Lighting Condition')\n",
    "\n",
    "    # Plot SHAP values distribution for each severity class\n",
    "    for i in range(5):\n",
    "        axes[i].hist(shap_values[i][:, light_feature_index], bins=20, alpha=0.5, label='Original')\n",
    "        axes[i].hist(shap_values_intervention[i][:, light_feature_index], bins=20, alpha=0.5, label='Intervention')\n",
    "        axes[i].set_title(f'Severity {severity_classes[i]}')\n",
    "        axes[i].set_xlabel('SHAP Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Intervention mapping: from 'Dark-Not lighted' to 'Dark-Lighted'\n",
    "intervention_mapping = {'from': 1, 'to': 0}\n",
    "\n",
    "# Call the function\n",
    "severity_classes = ['Possible injury', 'Minor injury', 'No injury', 'Serious injury', 'Fatal']  \n",
    "simulate_policy_intervention_and_plot_shap(X_test, shap_values, light_feature_index, severity_classes, intervention_mapping)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
