{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''\n",
    "# Pedestrian crash severity analysis using Stacking Ensemble Models\n",
    "# Author:   Amir Rafe (amir.rafe@usu.edu)\n",
    "# File:     StackingEnsembleAIM.ipynb\n",
    "# Date:     Spring 2024\n",
    "# Version:  1.02  \n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score , f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import shap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "Ped_df = pd.read_csv('df.csv')\n",
    "\n",
    "# List of features to use for modeling\n",
    "features = ['PersonType','Sex','AgeText','Aggressive','AlcoholSuspected','AlcResult',\n",
    "                       'UrbanRural','FunctionClass','CommercialMotorVehInvolved','Light',\n",
    "                       'Weather','RoadwaySurf','DisregardTrafficControl','DistractedDriving',\n",
    "                       'DomesticAnimalRelated','DrowsyDriving','DrugsSuspected','OlderDriverInvolved',\n",
    "                       'TeenageDriverInvolved','DUI','HeavyTruckInvolved','OverturnRollover','RightTurn',\n",
    "                       'TransitVehicleInvolved','HolidayCrash','HolidayCrashYN','Intersection',\n",
    "                       'LeftUTurnInvolved','VerticalAlignment','WorkZoneInvolved','WrongWayDriving']\n",
    "\n",
    "# Handle any NaN values\n",
    "Ped_df = Ped_df.fillna(0)\n",
    "Ped_df['Severity'] = Ped_df['Severity'].astype(int)\n",
    "\n",
    "# Split data into features (X) and label (y)\n",
    "X = Ped_df[features].values\n",
    "y = Ped_df['Severity'].values\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=25)\n",
    "\n",
    "# Hyperparameter tunning \n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [2, 4, 6],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1, 1.5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "xgb_model = XGBClassifier(num_class=5 , base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, eval_metric='mlogloss',\n",
    "              gamma=0, gpu_id=-1, importance_type='gain',\n",
    "              interaction_constraints='', learning_rate=0.300000012,\n",
    "              max_delta_step=0, max_depth=6, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
    "              num_parallel_tree=1, objective='multi:softprob', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
    "              tree_method='exact', use_label_encoder=False,\n",
    "              validate_parameters=1, verbosity=None)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \",grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=5000)))\n",
    "level0.append(('xgb', XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=5,\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.5,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=1.0)))\n",
    "level0.append(('et', ExtraTreesClassifier()))\n",
    "\n",
    "# Define meta learner model\n",
    "level1 = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=5000)\n",
    "\n",
    "# Define the stacking ensemble\n",
    "model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "\n",
    "# Fit the model on all scaled available data\n",
    "scaler = StandardScaler()\n",
    "X_res_scaled = scaler.fit_transform(X_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Make predictions\n",
    "model.fit(X_res_scaled, y_res)\n",
    "preds = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Evaluate the model using F1-score\n",
    "f1 = f1_score(y_test, preds, average='weighted')\n",
    "print(\"F1-score: %.2f\" % f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP interpretation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a background dataset from features. This subset is used as a reference point to compute SHAP values.\n",
    "# 150 samples are typically enough to approximate the SHAP values without being too computationally intensive.\n",
    "background = shap.sample(X_train, 150)\n",
    "\n",
    "# Initialize a SHAP explainer object. This uses the KernelExplainer, which is model-agnostic and can be used \n",
    "# with any machine learning model. The explainer requires a prediction function and a background dataset.\n",
    "# Here, `best_model.predict_proba` is passed to compute SHAP values for classification models, which outputs\n",
    "# probabilities for each class. The background dataset helps in comparing the feature's effect when present vs. absent.\n",
    "explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "\n",
    "# Compute SHAP values for each feature in dataset X. SHAP values quantify the impact of each feature on the model's prediction.\n",
    "# This computation is done for all instances in X, allowing to interpret the model's behavior. The `n_jobs=-1` parameter\n",
    "# enables parallel computation, using all available CPUs to speed up the calculation.\n",
    "shap_values = explainer.shap_values(background, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap summary plot (feature importance)\n",
    "\n",
    "features2 = ['PersonType','Sex','Age','Aggressive','DrugResult','AlcResult','UrbanRural','FunctionClass','CommercialVehInvolved',\n",
    "                       'LightingCondition','Weather','RoadwaySurf','DisregardTrafficControl','DistractedDriving','DomesticAnimalRelated',\n",
    "                       'DrowsyDriving','DrugsSuspected','OlderDriverInvolved','TeenageDriverInvolved','DUI','HeavyTruckInvolved',\n",
    "                       'OverturnRollover','RightTurnInvolved','TransitVehicleInvolved','Aggressive',\n",
    "                       'HolidayCrashYN','Intersection','LeftUTurnInvolved','VerticalAlignment','WorkZoneInvolved','WrongWayDriving']\n",
    "\n",
    "shap.summary_plot(shap_values, X_train, feature_names=features2 , plot_type= 'bar' , plot_size=(12,12) ,  class_names= {\n",
    "    0: 'Possible injury',\n",
    "    1: 'Minor injury',\n",
    "    2: 'No injury/PDO',\n",
    "    3: 'Serious injury',\n",
    "    4: 'Fatal'\n",
    "},show=False)\n",
    "\n",
    "# Get the current figure and axes objects\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "# Modifying main plot parameters\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"mean(|SHAP value|) (average impact on model output magnitude)\", fontsize=16 ,  labelpad=15)\n",
    "\n",
    "# Get colorbar\n",
    "plt.legend(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary_Stacking.png\",dpi=300) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap summary plot (dot plot)\n",
    "\n",
    "shap.summary_plot(shap_values[0], X_train, feature_names=features2 , plot_type= 'dot' , plot_size=(10,10),show=False)\n",
    "\n",
    "# Get the current figure and axes objects\n",
    "fig, ax = plt.gcf(), plt.gca()\n",
    "\n",
    "# Modifying main plot parameters\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel(\"SHAP value (impact on model output)\", fontsize=16  , labelpad=15)\n",
    "\n",
    "# Get colorbar\n",
    "cb_ax = fig.axes[1] \n",
    "cb_ax.tick_params(labelsize=16)\n",
    "cb_ax.set_ylabel(\"Feature value\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary_Stacking_Possible.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
